# üêç PYTHON BRIDGE INTEGRATION STRATEGY
## LaTeX Perfectionist v24-R3: OCaml-Python FFI Design

### üìã EXECUTIVE SUMMARY

**GOAL**: Create a seamless Python bridge to the formally verified OCaml incremental lexer while maintaining performance and correctness guarantees.

**ARCHITECTURE**: OCaml shared library (.so) with C FFI exports called from Python via ctypes

**PERFORMANCE TARGET**: <0.1ms FFI overhead for incremental operations  
**RELIABILITY TARGET**: 100% memory safe, graceful error handling, automatic fallback  
**INTEGRATION TARGET**: Drop-in replacement for existing batch tokenization

---

## üèóÔ∏è BRIDGE ARCHITECTURE OVERVIEW

### Component Stack
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Python Application                       ‚îÇ
‚îÇ                (Editor Integration)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ Python API
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Python Bridge Layer                        ‚îÇ
‚îÇ           (incr_bridge.py - Type Safety)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ ctypes FFI
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                C FFI Wrapper                           ‚îÇ
‚îÇ            (libincremental.so - Memory Safe)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ OCaml calling convention
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              OCaml Runtime                              ‚îÇ
‚îÇ         (Extracted from Coq - Formally Verified)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow
```
Python Edit Request
    ‚Üì (marshal to C types)
C FFI Layer  
    ‚Üì (convert to OCaml types)
OCaml Incremental Lexer
    ‚Üì (process with checkpoints)
OCaml Token List + State
    ‚Üì (convert to C types)
C FFI Layer
    ‚Üì (marshal to Python types)
Python Token Objects
```

---

## üîß OCAML SHARED LIBRARY DESIGN

### Library Structure

```ocaml
(* File: src/runtime/incremental_ffi.ml *)

open Incremental_lexer

(* C-compatible types for FFI *)
type c_token = {
  c_token_type: int;      (* Encoded token type *)
  c_content: string;      (* Token content *)
  c_line: int;           (* Line number *)
  c_column: int;         (* Column number *)
}

type c_edit_result = {
  c_tokens: c_token array;    (* Updated tokens *)
  c_token_count: int;         (* Number of tokens *)
  c_processing_time_ms: float; (* Performance metrics *)
  c_lines_processed: int;     (* Lines affected *)
  c_cache_hits: int;         (* Cache performance *)
  c_memory_usage_mb: float;   (* Memory monitoring *)
}

type c_lexer_state = {
  c_math_mode: bool;
  c_in_comment: bool; 
  c_buffer: string;
  c_line: int;
  c_column: int;
}

(* Global lexer instance management *)
let lexer_instances : (int, incremental_lexer) Hashtbl.t = Hashtbl.create 16
let next_lexer_id = ref 0

(* FFI-exported functions *)
external create_incremental_lexer : unit -> int = "ml_create_incremental_lexer"
external load_document : int -> string -> bool = "ml_load_document"
external apply_edit : int -> int -> int -> string -> c_edit_result = "ml_apply_edit"
external get_memory_usage : int -> float = "ml_get_memory_usage"
external clear_caches : int -> unit = "ml_clear_caches"
external destroy_lexer : int -> unit = "ml_destroy_lexer"

(* Implementation of exported functions *)
let ml_create_incremental_lexer () =
  let lexer_id = !next_lexer_id in
  incr next_lexer_id;
  let new_lexer = IncrementalLexer.create () in
  Hashtbl.add lexer_instances lexer_id new_lexer;
  lexer_id

let ml_load_document lexer_id content =
  match Hashtbl.find_opt lexer_instances lexer_id with
  | Some lexer ->
      (try
         IncrementalLexer.load_document lexer content;
         true
       with _ -> false)
  | None -> false

let ml_apply_edit lexer_id line column new_content =
  let start_time = Unix.gettimeofday () in
  match Hashtbl.find_opt lexer_instances lexer_id with
  | Some lexer ->
      (try
         let (updated_tokens, stats) = IncrementalLexer.apply_edit lexer line column new_content in
         let end_time = Unix.gettimeofday () in
         let processing_time = (end_time -. start_time) *. 1000.0 in
         
         (* Convert OCaml tokens to C-compatible format *)
         let c_tokens = Array.map (fun token ->
           {
             c_token_type = encode_token_type token.token_type;
             c_content = token.content;
             c_line = token.line;
             c_column = token.column;
           }
         ) (Array.of_list updated_tokens) in
         
         {
           c_tokens = c_tokens;
           c_token_count = Array.length c_tokens;
           c_processing_time_ms = processing_time;
           c_lines_processed = stats.lines_processed;
           c_cache_hits = stats.cache_hits;
           c_memory_usage_mb = IncrementalLexer.get_memory_usage lexer /. (1024.0 *. 1024.0);
         }
       with exn ->
         (* Error case - return empty result *)
         {
           c_tokens = [||];
           c_token_count = 0;
           c_processing_time_ms = -1.0; (* Indicates error *)
           c_lines_processed = 0;
           c_cache_hits = 0;
           c_memory_usage_mb = 0.0;
         })
  | None ->
      (* Invalid lexer ID *)
      {
        c_tokens = [||];
        c_token_count = 0;
        c_processing_time_ms = -1.0;
        c_lines_processed = 0;
        c_cache_hits = 0;
        c_memory_usage_mb = 0.0;
      }

let ml_get_memory_usage lexer_id =
  match Hashtbl.find_opt lexer_instances lexer_id with
  | Some lexer -> IncrementalLexer.get_memory_usage lexer /. (1024.0 *. 1024.0)
  | None -> 0.0

let ml_clear_caches lexer_id =
  match Hashtbl.find_opt lexer_instances lexer_id with
  | Some lexer -> IncrementalLexer.clear_caches lexer
  | None -> ()

let ml_destroy_lexer lexer_id =
  Hashtbl.remove lexer_instances lexer_id

(* Register FFI functions *)
let () =
  Callback.register "ml_create_incremental_lexer" ml_create_incremental_lexer;
  Callback.register "ml_load_document" ml_load_document;
  Callback.register "ml_apply_edit" ml_apply_edit;
  Callback.register "ml_get_memory_usage" ml_get_memory_usage;
  Callback.register "ml_clear_caches" ml_clear_caches;
  Callback.register "ml_destroy_lexer" ml_destroy_lexer
```

### C FFI Wrapper

```c
/* File: src/runtime/incremental_ffi.c */

#include <caml/mlvalues.h>
#include <caml/memory.h>
#include <caml/alloc.h>
#include <caml/callback.h>
#include <caml/fail.h>
#include <string.h>
#include <stdio.h>

/* C structures matching OCaml types */
typedef struct {
    int token_type;
    char* content;
    int line;
    int column;
} c_token_t;

typedef struct {
    c_token_t* tokens;
    int token_count;
    double processing_time_ms;
    int lines_processed;
    int cache_hits;
    double memory_usage_mb;
} c_edit_result_t;

/* Global initialization */
static int ocaml_initialized = 0;

void ensure_ocaml_initialized() {
    if (!ocaml_initialized) {
        char *argv[] = {"incremental_lexer", NULL};
        caml_startup(argv);
        ocaml_initialized = 1;
    }
}

/* Exported C functions for Python FFI */

int create_incremental_lexer() {
    ensure_ocaml_initialized();
    
    static value *create_func = NULL;
    if (create_func == NULL) {
        create_func = caml_named_value("ml_create_incremental_lexer");
    }
    
    CAMLparam0();
    CAMLlocal1(result);
    
    result = caml_callback(*create_func, Val_unit);
    
    CAMLreturnT(int, Int_val(result));
}

int load_document(int lexer_id, const char* content) {
    ensure_ocaml_initialized();
    
    static value *load_func = NULL;
    if (load_func == NULL) {
        load_func = caml_named_value("ml_load_document");
    }
    
    CAMLparam0();
    CAMLlocal3(ml_lexer_id, ml_content, result);
    
    ml_lexer_id = Val_int(lexer_id);
    ml_content = caml_copy_string(content);
    
    result = caml_callback2(*load_func, ml_lexer_id, ml_content);
    
    CAMLreturnT(int, Bool_val(result));
}

c_edit_result_t* apply_edit(int lexer_id, int line, int column, const char* new_content) {
    ensure_ocaml_initialized();
    
    static value *edit_func = NULL;
    if (edit_func == NULL) {
        edit_func = caml_named_value("ml_apply_edit");
    }
    
    CAMLparam0();
    CAMLlocal5(ml_lexer_id, ml_line, ml_column, ml_content, result);
    
    ml_lexer_id = Val_int(lexer_id);
    ml_line = Val_int(line);
    ml_column = Val_int(column);
    ml_content = caml_copy_string(new_content);
    
    result = caml_callback4(*edit_func, ml_lexer_id, ml_line, ml_column, ml_content);
    
    /* Convert OCaml result to C struct */
    c_edit_result_t* c_result = malloc(sizeof(c_edit_result_t));
    
    /* Extract fields from OCaml record */
    value tokens_array = Field(result, 0);
    int token_count = Int_val(Field(result, 1));
    double processing_time = Double_val(Field(result, 2));
    int lines_processed = Int_val(Field(result, 3));
    int cache_hits = Int_val(Field(result, 4));
    double memory_usage = Double_val(Field(result, 5));
    
    /* Allocate and populate tokens array */
    c_result->tokens = malloc(token_count * sizeof(c_token_t));
    c_result->token_count = token_count;
    c_result->processing_time_ms = processing_time;
    c_result->lines_processed = lines_processed;
    c_result->cache_hits = cache_hits;
    c_result->memory_usage_mb = memory_usage;
    
    /* Copy token data */
    for (int i = 0; i < token_count; i++) {
        value token = Field(tokens_array, i);
        c_result->tokens[i].token_type = Int_val(Field(token, 0));
        c_result->tokens[i].content = strdup(String_val(Field(token, 1)));
        c_result->tokens[i].line = Int_val(Field(token, 2));
        c_result->tokens[i].column = Int_val(Field(token, 3));
    }
    
    CAMLreturnT(c_edit_result_t*, c_result);
}

void free_edit_result(c_edit_result_t* result) {
    if (result) {
        if (result->tokens) {
            for (int i = 0; i < result->token_count; i++) {
                free(result->tokens[i].content);
            }
            free(result->tokens);
        }
        free(result);
    }
}

double get_memory_usage(int lexer_id) {
    ensure_ocaml_initialized();
    
    static value *memory_func = NULL;
    if (memory_func == NULL) {
        memory_func = caml_named_value("ml_get_memory_usage");
    }
    
    CAMLparam0();
    CAMLlocal2(ml_lexer_id, result);
    
    ml_lexer_id = Val_int(lexer_id);
    result = caml_callback(*memory_func, ml_lexer_id);
    
    CAMLreturnT(double, Double_val(result));
}

void destroy_lexer(int lexer_id) {
    ensure_ocaml_initialized();
    
    static value *destroy_func = NULL;
    if (destroy_func == NULL) {
        destroy_func = caml_named_value("ml_destroy_lexer");
    }
    
    CAMLparam0();
    CAMLlocal1(ml_lexer_id);
    
    ml_lexer_id = Val_int(lexer_id);
    caml_callback(*destroy_func, ml_lexer_id);
    
    CAMLreturn0;
}
```

---

## üêç PYTHON BRIDGE LAYER

### Core Bridge Implementation

```python
# File: src/python/incr_bridge.py

import ctypes
from ctypes import Structure, POINTER, c_int, c_double, c_char_p, c_bool
import os
import time
import logging
from typing import List, Optional, Dict, Any
from dataclasses import dataclass
from enum import IntEnum

# Load OCaml shared library
lib_path = os.path.join(os.path.dirname(__file__), '../runtime/libincremental.so')
if not os.path.exists(lib_path):
    raise ImportError(f"Incremental lexer library not found at {lib_path}")

incremental_lib = ctypes.CDLL(lib_path)

# Token type enumeration (must match OCaml encoding)
class TokenType(IntEnum):
    TEXT = 0
    COMMAND = 1
    MATH = 2
    COMMENT = 3
    SYMBOL = 4
    EOF = 5

# C structure definitions
class CToken(Structure):
    _fields_ = [
        ("token_type", c_int),
        ("content", c_char_p),
        ("line", c_int),
        ("column", c_int),
    ]

class CEditResult(Structure):
    _fields_ = [
        ("tokens", POINTER(CToken)),
        ("token_count", c_int),
        ("processing_time_ms", c_double),
        ("lines_processed", c_int),
        ("cache_hits", c_int),
        ("memory_usage_mb", c_double),
    ]

# Python token class
@dataclass
class Token:
    token_type: TokenType
    content: str
    line: int
    column: int
    
    def __eq__(self, other):
        if not isinstance(other, Token):
            return False
        return (self.token_type == other.token_type and
                self.content == other.content and
                self.line == other.line and
                self.column == other.column)

@dataclass  
class EditResult:
    tokens: List[Token]
    processing_time_ms: float
    lines_processed: int
    cache_hits: int
    memory_usage_mb: float
    
    @property
    def cache_hit_rate(self) -> float:
        total_lookups = self.cache_hits + max(1, self.lines_processed - self.cache_hits)
        return self.cache_hits / total_lookups

# Configure library function signatures
incremental_lib.create_incremental_lexer.restype = c_int
incremental_lib.create_incremental_lexer.argtypes = []

incremental_lib.load_document.restype = c_bool
incremental_lib.load_document.argtypes = [c_int, c_char_p]

incremental_lib.apply_edit.restype = POINTER(CEditResult)
incremental_lib.apply_edit.argtypes = [c_int, c_int, c_int, c_char_p]

incremental_lib.get_memory_usage.restype = c_double
incremental_lib.get_memory_usage.argtypes = [c_int]

incremental_lib.destroy_lexer.restype = None
incremental_lib.destroy_lexer.argtypes = [c_int]

incremental_lib.free_edit_result.restype = None
incremental_lib.free_edit_result.argtypes = [POINTER(CEditResult)]

class IncrementalLexerError(Exception):
    """Base exception for incremental lexer errors"""
    pass

class IncrementalLexer:
    """Python interface to formally verified incremental LaTeX lexer"""
    
    def __init__(self, max_memory_mb: int = 100):
        self.lexer_id = incremental_lib.create_incremental_lexer()
        self.max_memory_mb = max_memory_mb
        self.current_document = None
        self.performance_stats = {
            'total_edits': 0,
            'total_time_ms': 0.0,
            'cache_hit_rate': 0.0,
            'memory_high_water_mb': 0.0
        }
        
        if self.lexer_id < 0:
            raise IncrementalLexerError("Failed to create incremental lexer instance")
    
    def load_document(self, content: str) -> bool:
        """Load LaTeX document for incremental processing"""
        
        if not isinstance(content, str):
            raise TypeError("Document content must be a string")
        
        try:
            # Encode content as UTF-8 bytes
            content_bytes = content.encode('utf-8')
            
            success = incremental_lib.load_document(self.lexer_id, content_bytes)
            if success:
                self.current_document = content
                logging.info(f"Document loaded: {len(content)} characters, {content.count('\\n')} lines")
                return True
            else:
                logging.error("Failed to load document into incremental lexer")
                return False
                
        except Exception as e:
            logging.error(f"Error loading document: {e}")
            raise IncrementalLexerError(f"Document loading failed: {e}")
    
    def apply_edit(self, line: int, column: int, new_content: str) -> EditResult:
        """Apply incremental edit and return updated tokens"""
        
        if self.current_document is None:
            raise IncrementalLexerError("No document loaded")
        
        if not isinstance(new_content, str):
            raise TypeError("Edit content must be a string")
        
        if line < 0 or column < 0:
            raise ValueError("Line and column must be non-negative")
        
        start_time = time.perf_counter()
        
        try:
            # Call OCaml function
            content_bytes = new_content.encode('utf-8')
            c_result_ptr = incremental_lib.apply_edit(self.lexer_id, line, column, content_bytes)
            
            if not c_result_ptr:
                raise IncrementalLexerError("OCaml function returned null pointer")
            
            # Dereference pointer to get result structure
            c_result = c_result_ptr.contents
            
            # Check for error condition
            if c_result.processing_time_ms < 0:
                incremental_lib.free_edit_result(c_result_ptr)
                raise IncrementalLexerError("Incremental processing failed in OCaml layer")
            
            # Convert C tokens to Python objects
            tokens = []
            for i in range(c_result.token_count):
                c_token = c_result.tokens[i]
                token = Token(
                    token_type=TokenType(c_token.token_type),
                    content=c_token.content.decode('utf-8') if c_token.content else "",
                    line=c_token.line,
                    column=c_token.column
                )
                tokens.append(token)
            
            # Create result object
            result = EditResult(
                tokens=tokens,
                processing_time_ms=c_result.processing_time_ms,
                lines_processed=c_result.lines_processed,
                cache_hits=c_result.cache_hits,
                memory_usage_mb=c_result.memory_usage_mb
            )
            
            # Free C memory
            incremental_lib.free_edit_result(c_result_ptr)
            
            # Update performance statistics
            self._update_performance_stats(result)
            
            # Check memory usage
            if result.memory_usage_mb > self.max_memory_mb:
                logging.warning(f"Memory usage {result.memory_usage_mb:.1f}MB exceeds limit {self.max_memory_mb}MB")
            
            # Log performance if slow
            total_time = (time.perf_counter() - start_time) * 1000
            if total_time > 10.0:  # >10ms
                logging.warning(f"Slow edit: {total_time:.2f}ms total, {result.processing_time_ms:.2f}ms OCaml")
            
            return result
            
        except Exception as e:
            logging.error(f"Error in apply_edit: {e}")
            # Try to free memory if we have a pointer
            if 'c_result_ptr' in locals() and c_result_ptr:
                try:
                    incremental_lib.free_edit_result(c_result_ptr)
                except:
                    pass
            raise IncrementalLexerError(f"Edit application failed: {e}")
    
    def get_memory_usage(self) -> float:
        """Get current memory usage in MB"""
        try:
            return incremental_lib.get_memory_usage(self.lexer_id)
        except Exception as e:
            logging.error(f"Error getting memory usage: {e}")
            return 0.0  
    
    def clear_caches(self):
        """Clear all internal caches (for memory pressure relief)"""
        try:
            incremental_lib.clear_caches(self.lexer_id)
            logging.info("Incremental lexer caches cleared")
        except Exception as e:
            logging.error(f"Error clearing caches: {e}")
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get performance statistics"""
        return self.performance_stats.copy()
    
    def _update_performance_stats(self, result: EditResult):
        """Update internal performance tracking"""
        self.performance_stats['total_edits'] += 1
        self.performance_stats['total_time_ms'] += result.processing_time_ms
        self.performance_stats['cache_hit_rate'] = (
            (self.performance_stats['cache_hit_rate'] * (self.performance_stats['total_edits'] - 1) + 
             result.cache_hit_rate) / self.performance_stats['total_edits']
        )
        self.performance_stats['memory_high_water_mb'] = max(
            self.performance_stats['memory_high_water_mb'],
            result.memory_usage_mb
        )
    
    def __del__(self):
        """Cleanup OCaml resources"""
        if hasattr(self, 'lexer_id'):
            try:
                incremental_lib.destroy_lexer(self.lexer_id)
            except:
                pass  # Ignore cleanup errors

# Factory function for backward compatibility
def create_incremental_lexer(**kwargs) -> IncrementalLexer:
    """Create new incremental lexer instance"""
    return IncrementalLexer(**kwargs)
```

### High-Level Integration Layer

```python
# File: src/python/editor_integration.py

import time
import logging
from typing import List, Optional, Callable, Dict, Any
from dataclasses import dataclass
from .incr_bridge import IncrementalLexer, Token, IncrementalLexerError

# Import existing batch processing for fallback
try:
    from production_validator import batch_tokenize
    BATCH_FALLBACK_AVAILABLE = True
except ImportError:
    BATCH_FALLBACK_AVAILABLE = False
    logging.warning("Batch tokenizer not available - no fallback option")

@dataclass
class EditOperation:
    line: int
    column: int
    content: str
    timestamp: float = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()

class RealTimeLaTeXLexer:
    """High-level interface for real-time LaTeX editing"""
    
    def __init__(self, 
                 max_memory_mb: int = 100,
                 performance_threshold_ms: float = 100.0,
                 enable_fallback: bool = True,
                 enable_monitoring: bool = True):
        
        self.incremental_lexer = IncrementalLexer(max_memory_mb=max_memory_mb)
        self.performance_threshold_ms = performance_threshold_ms
        self.enable_fallback = enable_fallback and BATCH_FALLBACK_AVAILABLE
        self.enable_monitoring = enable_monitoring
        
        self.current_document = None
        self.edit_history: List[EditOperation] = []
        self.performance_monitor = PerformanceMonitor() if enable_monitoring else None
        self.fallback_mode = False
        
        logging.info(f"RealTimeLaTeXLexer initialized: fallback={'‚úÖ' if self.enable_fallback else '‚ùå'}")
    
    def set_document(self, latex_content: str):
        """Load new LaTeX document for real-time editing"""
        
        start_time = time.perf_counter()
        
        try:
            success = self.incremental_lexer.load_document(latex_content)
            if not success:
                raise IncrementalLexerError("Failed to load document")
            
            self.current_document = latex_content
            self.edit_history.clear()
            self.fallback_mode = False
            
            load_time = (time.perf_counter() - start_time) * 1000
            
            logging.info(f"Document loaded: {len(latex_content)} chars, {latex_content.count('\\n')} lines in {load_time:.2f}ms")
            
        except Exception as e:
            logging.error(f"Document loading failed: {e}")
            if self.enable_fallback:
                logging.info("Enabling fallback mode for this document")
                self.fallback_mode = True
                self.current_document = latex_content
            else:
                raise
    
    def on_text_change(self, line: int, column: int, new_content: str) -> List[Token]:
        """Handle real-time text changes with automatic fallback"""
        
        if self.current_document is None:
            raise ValueError("No document loaded")
        
        edit_op = EditOperation(line, column, new_content)
        self.edit_history.append(edit_op)
        
        # Use fallback mode if enabled
        if self.fallback_mode:
            return self._fallback_tokenize(edit_op)
        
        # Try incremental processing
        try:
            return self._incremental_tokenize(edit_op)
            
        except Exception as e:
            logging.error(f"Incremental processing failed: {e}")
            
            if self.enable_fallback:
                logging.info("Falling back to batch processing")
                self.fallback_mode = True
                return self._fallback_tokenize(edit_op)
            else:
                raise
    
    def _incremental_tokenize(self, edit_op: EditOperation) -> List[Token]:
        """Process edit using incremental lexer"""
        
        start_time = time.perf_counter()
        
        try:
            result = self.incremental_lexer.apply_edit(
                edit_op.line, 
                edit_op.column, 
                edit_op.content
            )
            
            total_time = (time.perf_counter() - start_time) * 1000
            
            # Performance monitoring
            if self.enable_monitoring:
                self.performance_monitor.record_edit(
                    edit_type='incremental',
                    file_size=len(self.current_document),
                    response_time_ms=total_time,
                    ocaml_time_ms=result.processing_time_ms,
                    lines_processed=result.lines_processed,
                    cache_hit_rate=result.cache_hit_rate,
                    memory_usage_mb=result.memory_usage_mb
                )
            
            # Performance warning
            if total_time > self.performance_threshold_ms:
                logging.warning(f"Slow incremental edit: {total_time:.2f}ms (threshold: {self.performance_threshold_ms}ms)")
            
            # Memory pressure check
            if result.memory_usage_mb > self.incremental_lexer.max_memory_mb * 0.9:
                logging.warning("Memory pressure detected - consider clearing caches")
                self.incremental_lexer.clear_caches()
            
            return result.tokens
            
        except IncrementalLexerError as e:
            logging.error(f"Incremental lexer error: {e}")
            raise
    
    def _fallback_tokenize(self, edit_op: EditOperation) -> List[Token]:
        """Process edit using batch tokenizer fallback"""
        
        if not BATCH_FALLBACK_AVAILABLE:
            raise IncrementalLexerError("Fallback processing not available")
        
        start_time = time.perf_counter()
        
        try:
            # Apply edit to document (simplified - would need proper edit application)
            updated_document = self._apply_edit_to_document(edit_op)
            
            # Use batch tokenizer
            batch_tokens = batch_tokenize(updated_document)
            
            # Convert to our Token format (assuming compatible)
            tokens = [Token(
                token_type=token.type,
                content=token.content,
                line=token.line,
                column=token.column
            ) for token in batch_tokens]
            
            total_time = (time.perf_counter() - start_time) * 1000
            
            # Performance monitoring
            if self.enable_monitoring:
                self.performance_monitor.record_edit(
                    edit_type='fallback_batch',
                    file_size=len(updated_document),
                    response_time_ms=total_time,
                    ocaml_time_ms=0.0,
                    lines_processed=updated_document.count('\n'),
                    cache_hit_rate=0.0,
                    memory_usage_mb=0.0
                )
            
            logging.info(f"Fallback processing: {total_time:.2f}ms for {len(tokens)} tokens")
            
            return tokens
            
        except Exception as e:
            logging.error(f"Fallback processing failed: {e}")
            raise IncrementalLexerError(f"Both incremental and fallback processing failed: {e}")
    
    def _apply_edit_to_document(self, edit_op: EditOperation) -> str:
        """Apply edit operation to current document (simplified implementation)"""
        
        lines = self.current_document.split('\n')
        
        if edit_op.line < len(lines):
            line_content = lines[edit_op.line]
            if edit_op.column <= len(line_content):
                # Simple insertion (would need more sophisticated edit handling)
                lines[edit_op.line] = (line_content[:edit_op.column] + 
                                     edit_op.content + 
                                     line_content[edit_op.column:])
        
        return '\n'.join(lines)
    
    def get_performance_report(self) -> Dict[str, Any]:
        """Get comprehensive performance report"""
        
        report = {
            'incremental_stats': self.incremental_lexer.get_performance_stats(),
            'fallback_mode': self.fallback_mode,
            'total_edits': len(self.edit_history),
        }
        
        if self.enable_monitoring:
            report['detailed_metrics'] = self.performance_monitor.get_report()
        
        return report

class PerformanceMonitor:
    """Monitor and analyze real-time editing performance"""
    
    def __init__(self):
        self.edit_records = []
        self.performance_alerts = []
    
    def record_edit(self, edit_type: str, file_size: int, response_time_ms: float, 
                   ocaml_time_ms: float, lines_processed: int, 
                   cache_hit_rate: float, memory_usage_mb: float):
        
        record = {
            'timestamp': time.time(),
            'edit_type': edit_type,
            'file_size': file_size,
            'response_time_ms': response_time_ms,
            'ocaml_time_ms': ocaml_time_ms,
            'lines_processed': lines_processed,
            'cache_hit_rate': cache_hit_rate,
            'memory_usage_mb': memory_usage_mb,
        }
        
        self.edit_records.append(record)
        
        # Performance alerting
        if response_time_ms > 100:  # Real-time threshold
            alert = {
                'timestamp': record['timestamp'],
                'type': 'slow_response',
                'message': f"Slow response: {response_time_ms:.2f}ms",
                'record': record
            }
            self.performance_alerts.append(alert)
    
    def get_report(self) -> Dict[str, Any]:
        """Generate comprehensive performance report"""
        
        if not self.edit_records:
            return {'message': 'No edit records available'}
        
        # Calculate statistics
        response_times = [r['response_time_ms'] for r in self.edit_records]
        ocaml_times = [r['ocaml_time_ms'] for r in self.edit_records]
        cache_hit_rates = [r['cache_hit_rate'] for r in self.edit_records if r['cache_hit_rate'] > 0]
        
        return {
            'total_edits': len(self.edit_records),
            'performance_alerts': len(self.performance_alerts),
            'response_time_stats': {
                'average_ms': sum(response_times) / len(response_times),
                'max_ms': max(response_times),
                'p95_ms': sorted(response_times)[int(len(response_times) * 0.95)],
            },
            'ocaml_time_stats': {
                'average_ms': sum(ocaml_times) / len(ocaml_times),
                'max_ms': max(ocaml_times),
            },
            'cache_performance': {
                'average_hit_rate': sum(cache_hit_rates) / len(cache_hit_rates) if cache_hit_rates else 0,
            },
            'memory_usage': {
                'max_mb': max(r['memory_usage_mb'] for r in self.edit_records),
                'average_mb': sum(r['memory_usage_mb'] for r in self.edit_records) / len(self.edit_records),
            }
        }
```

---

## üîß BUILD SYSTEM INTEGRATION

### Makefile for Shared Library

```makefile
# File: src/runtime/Makefile

OCAML_FLAGS = -I +str -I ../extraction
C_FLAGS = -fPIC -shared -I$(shell ocaml -where)
OCAML_LIBS = str.cmxa unix.cmxa

# Source files
OCAML_SOURCES = incremental_lexer.ml incremental_ffi.ml
C_SOURCES = incremental_ffi.c
EXTRACTED_SOURCES = $(wildcard ../extraction/*.ml)

# Output targets
SHARED_LIB = libincremental.so
OCAML_OBJECTS = $(OCAML_SOURCES:.ml=.cmx)
C_OBJECTS = $(C_SOURCES:.c=.o)

.PHONY: all clean install

all: $(SHARED_LIB)

$(SHARED_LIB): $(OCAML_OBJECTS) $(C_OBJECTS)
	ocamlopt -output-obj -o incremental_core.o $(OCAML_LIBS) $(EXTRACTED_SOURCES:.ml=.cmx) $(OCAML_OBJECTS)
	gcc $(C_FLAGS) -o $(SHARED_LIB) incremental_core.o $(C_OBJECTS) -lm -ldl

%.cmx: %.ml
	ocamlopt $(OCAML_FLAGS) -c $<

%.o: %.c
	gcc $(C_FLAGS) -c $<

clean:
	rm -f *.cmx *.cmi *.o $(SHARED_LIB) incremental_core.o

install: $(SHARED_LIB)
	cp $(SHARED_LIB) ../python/

# Development targets
test-ffi: $(SHARED_LIB)
	cd ../python && python -c "import incr_bridge; print('FFI test passed')"

profile: $(SHARED_LIB)
	cd ../python && python -m cProfile -o incremental.prof test_performance.py

debug: OCAML_FLAGS += -g
debug: C_FLAGS += -g -DDEBUG
debug: $(SHARED_LIB)
```

### Python Setup Script

```python
# File: src/python/setup.py

from setuptools import setup, Extension
import os

# Build extension module
setup(
    name="latex_perfectionist_incremental",
    version="24.3.0",
    description="Real-time incremental LaTeX lexer with formal verification",
    author="LaTeX Perfectionist Team",
    
    packages=["latex_perfectionist"],
    package_dir={"latex_perfectionist": "."},
    
    # Include shared library
    package_data={
        "latex_perfectionist": ["libincremental.so"]
    },
    
    install_requires=[
        "typing-extensions>=3.7.4",
    ],
    
    python_requires=">=3.7",
    
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Developers",
        "License :: OSI Approved :: MIT License", 
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.7",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Topic :: Text Processing :: Markup :: LaTeX",
        "Topic :: Software Development :: Libraries :: Python Modules",
    ],
)
```

---

## üß™ TESTING AND VALIDATION STRATEGY

### FFI Testing

```python
# File: src/python/tests/test_ffi_bridge.py

import unittest
import time
import os
import tempfile
from unittest.mock import patch, MagicMock

from incr_bridge import IncrementalLexer, Token, TokenType, IncrementalLexerError

class TestFFIBridge(unittest.TestCase):
    
    def setUp(self):
        """Set up test environment"""
        self.lexer = IncrementalLexer()
        self.test_document = """
\\documentclass{article}
\\begin{document}
Hello $x^2$ world % comment
\\section{Test}
More content here.
\\end{document}
        """.strip()
    
    def tearDown(self):
        """Clean up test environment"""
        del self.lexer
    
    def test_lexer_creation(self):
        """Test lexer instance creation"""
        self.assertIsInstance(self.lexer, IncrementalLexer)
        self.assertGreaterEqual(self.lexer.lexer_id, 0)
    
    def test_document_loading(self):
        """Test document loading functionality"""
        success = self.lexer.load_document(self.test_document)
        self.assertTrue(success)
        self.assertEqual(self.lexer.current_document, self.test_document)
    
    def test_incremental_edit(self):
        """Test basic incremental editing"""
        # Load document
        self.lexer.load_document(self.test_document)
        
        # Apply edit
        result = self.lexer.apply_edit(2, 6, "beautiful ")
        
        # Validate result structure
        self.assertIsNotNone(result)
        self.assertIsInstance(result.tokens, list)
        self.assertGreater(len(result.tokens), 0)
        self.assertGreaterEqual(result.processing_time_ms, 0)
        self.assertGreaterEqual(result.lines_processed, 0)
        
        # Validate token structure
        for token in result.tokens:
            self.assertIsInstance(token, Token)
            self.assertIsInstance(token.token_type, TokenType)
            self.assertIsInstance(token.content, str)
            self.assertIsInstance(token.line, int)
            self.assertIsInstance(token.column, int)
    
    def test_memory_usage_tracking(self):
        """Test memory usage monitoring"""
        self.lexer.load_document(self.test_document)
        
        initial_memory = self.lexer.get_memory_usage()
        self.assertGreaterEqual(initial_memory, 0)
        
        # Apply several edits
        for i in range(10):
            self.lexer.apply_edit(1, 0, f"edit_{i} ")
        
        final_memory = self.lexer.get_memory_usage()
        self.assertGreaterEqual(final_memory, initial_memory)
    
    def test_error_handling(self):
        """Test error handling in FFI layer"""
        
        # Test edit without document
        with self.assertRaises(IncrementalLexerError):
            self.lexer.apply_edit(0, 0, "test")
        
        # Test invalid edit positions
        self.lexer.load_document(self.test_document)
        
        with self.assertRaises(ValueError):
            self.lexer.apply_edit(-1, 0, "test")
        
        with self.assertRaises(ValueError):
            self.lexer.apply_edit(0, -1, "test")
    
    def test_performance_requirements(self):
        """Test performance meets requirements"""
        self.lexer.load_document(self.test_document * 100)  # Larger document
        
        # Single character edit
        start_time = time.perf_counter()
        result = self.lexer.apply_edit(50, 10, 'x')
        elapsed_ms = (time.perf_counter() - start_time) * 1000
        
        # Should be much faster than batch processing
        self.assertLess(elapsed_ms, 10.0)  # <10ms total time
        self.assertLess(result.processing_time_ms, 5.0)  # <5ms OCaml time
    
    def test_memory_cleanup(self):
        """Test proper memory cleanup"""
        initial_memory = self.lexer.get_memory_usage()
        
        # Create many lexer instances
        lexers = []
        for i in range(10):
            lexer = IncrementalLexer()
            lexer.load_document(self.test_document)
            lexers.append(lexer)
        
        # Clean them up
        for lexer in lexers:
            del lexer
        
        # Memory should not grow unboundedly
        final_memory = self.lexer.get_memory_usage()
        self.assertLess(final_memory, initial_memory + 50.0)  # <50MB growth

class TestIntegrationLayer(unittest.TestCase):
    """Test high-level integration layer"""
    
    def setUp(self):
        self.editor = RealTimeLaTeXLexer(enable_monitoring=True)
        self.test_document = """
\\documentclass{article}
\\begin{document}
Hello world!
\\end{document}
        """.strip()
    
    def test_document_loading(self):
        """Test document loading through integration layer"""
        self.editor.set_document(self.test_document)
        self.assertEqual(self.editor.current_document, self.test_document)
        self.assertFalse(self.editor.fallback_mode)
    
    def test_real_time_editing(self):
        """Test real-time editing workflow"""
        self.editor.set_document(self.test_document)
        
        # Apply edit
        tokens = self.editor.on_text_change(2, 6, "beautiful ")
        
        # Validate response
        self.assertIsInstance(tokens, list)
        self.assertGreater(len(tokens), 0)
        
        # Check performance monitoring
        report = self.editor.get_performance_report()
        self.assertIn('incremental_stats', report)
        self.assertGreater(report['total_edits'], 0)
    
    @patch('editor_integration.BATCH_FALLBACK_AVAILABLE', True)
    @patch('editor_integration.batch_tokenize')
    def test_fallback_mode(self, mock_batch_tokenize):
        """Test automatic fallback to batch processing"""
        
        # Mock batch tokenize to return test tokens
        mock_batch_tokenize.return_value = [
            MagicMock(type=TokenType.TEXT, content="test", line=0, column=0)
        ]
        
        # Force fallback mode
        self.editor.fallback_mode = True
        self.editor.current_document = self.test_document
        
        # Apply edit
        tokens = self.editor.on_text_change(0, 0, "test")
        
        # Verify fallback was used
        mock_batch_tokenize.assert_called_once()
        self.assertIsInstance(tokens, list)

if __name__ == '__main__':
    unittest.main()
```

---

## üéØ INTEGRATION SUCCESS METRICS

### Performance Targets
- [ ] **FFI Overhead**: <0.1ms per edit operation
- [ ] **Memory Efficiency**: No memory leaks across language boundaries
- [ ] **Error Handling**: Graceful failure and automatic fallback
- [ ] **Type Safety**: No segmentation faults or type errors

### Reliability Targets
- [ ] **24/7 Operation**: Stable under continuous editing
- [ ] **Large Files**: Handle 10MB+ documents without issues
- [ ] **Memory Pressure**: Graceful degradation under memory limits
- [ ] **Error Recovery**: Automatic recovery from OCaml exceptions

### Integration Targets
- [ ] **Drop-in Replacement**: Compatible with existing Python code
- [ ] **Performance Monitoring**: Real-time performance visibility
- [ ] **Fallback Mode**: Seamless fallback to batch processing
- [ ] **Editor Integration**: Easy integration with text editors

---

## üèÜ FINAL INTEGRATION PROMISE

**PERFORMANCE PROMISE**: The Python bridge will add <0.1ms overhead to the formally verified OCaml incremental lexer while providing full Python integration.

**RELIABILITY PROMISE**: The bridge will handle all error conditions gracefully, automatically falling back to batch processing when needed without data loss.

**USABILITY PROMISE**: The integration layer will provide a simple Python API that works as a drop-in replacement for existing batch tokenization with dramatically improved performance.

**This bridge connects mathematical perfection with practical usability.** üêç‚ö°